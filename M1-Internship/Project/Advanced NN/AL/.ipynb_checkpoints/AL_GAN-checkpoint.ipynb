{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f38a5f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49305665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 10:59:54.550731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-14 10:59:54.550760: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colorbar import Colorbar\n",
    "\n",
    "\n",
    "plt.style.use(['science','ieee'])\n",
    "\n",
    "os.chdir('/home/mathis/Stage M1/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73bed5",
   "metadata": {},
   "source": [
    "# Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63516bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GeV [min,max]\n",
    "\n",
    "M1_range = M2_range = M3_range = [50, 5000]\n",
    "mA_range = [50, 5000]\n",
    "tanB_range = [2, 60]\n",
    "mu_range = [-10**4, 10**4]\n",
    "At_range = Ab_range = Atau_range = [-10**4, 10**4]\n",
    "Mq1L_range = Mq3L_range = [50, 5000]\n",
    "MuR_range = MdR_range = MtR_range = MbR_range = [50, 5000]\n",
    "MeL_range = MtauL_range = MeR_range = MtauR_range = [50, 5000]\n",
    "\n",
    "PMSSM_range_big= [M1_range, M2_range, M3_range, mA_range, tanB_range, mu_range, At_range, Ab_range, \n",
    "              Atau_range, Mq1L_range, Mq3L_range, MuR_range, MdR_range, MtR_range, MbR_range,\n",
    "             MeL_range, MtauL_range, MeR_range, MtauR_range]\n",
    "\n",
    "PMSSM_min = np.array([i[0] for i in PMSSM_range_big])\n",
    "PMSSM_max = np.array([i[1] for i in PMSSM_range_big])\n",
    "\n",
    "PMSSM_sub = np.subtract(PMSSM_max, PMSSM_min)\n",
    "\n",
    "PMSSM_range = [PMSSM_min,PMSSM_max,PMSSM_sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd866885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_slha(PMSSM_NN, nom_slha):\n",
    "    f = open(nom_slha, \"w\")\n",
    "    \n",
    "    f.write(\"BLOCK MODSEL \\n\")\n",
    "    f.write(\" 1    0              #MSSM \\n\")\n",
    "    \n",
    "    f.write(\"BLOCK SMINPUTS \\n\")\n",
    "    f.write(\"  1   1.279340000e+02     # alpha^(-1) SM MSbar(M1) \\n\")\n",
    "    f.write(\"  2   1.663700000e-05     # G_Fermi \\n\")\n",
    "    f.write(\"  3   0.1179              # alphas(MS) SM MSbar \\n\")\n",
    "    f.write(\"  4   91.1876             # MZ(pole) \\n\")\n",
    "    f.write(\"  5   4.18                # mb(mb) SM MSbar \\n\")\n",
    "    f.write(\"  6   172.9               # mtop(pole) \\n\")\n",
    "    f.write(\"  7   1.776860000e+00     #m mtau(pole) \\n\")\n",
    "    \n",
    "    f.write(\"BLOCK MINPAR \\n\")\n",
    "    f.write(\"  3   \"+str(PMSSM_NN[4])+\"   #tanb \\n\")\n",
    "    \n",
    "    f.write(\"BLOCK VCKMIN   #CKM param (Wolfenstein) \\n\")\n",
    "    f.write(\"  1   0.22650  #lambda \\n\")\n",
    "    f.write(\"  2   0.790    #A \\n\")\n",
    "    f.write(\"  3   0.141    #rho \\n\")\n",
    "    f.write(\"  4   0.357    #eta \\n\")\n",
    "    \n",
    "    f.write(\"BLOCK EXTPAR \\n\")\n",
    "    f.write(\"  0   \"+str(-1.0)+\"               # Q \\n\")\n",
    "    f.write(\"  1   \"+str(PMSSM_NN[0])+\"        # M1 \\n\")\n",
    "    f.write(\"  2   \"+str(PMSSM_NN[1])+\"        # M2 \\n\")\n",
    "    f.write(\"  3   \"+str(PMSSM_NN[2])+\"        # M3 \\n\")\n",
    "    f.write(\"  11   \"+str(PMSSM_NN[6])+\"       # At \\n\")\n",
    "    f.write(\"  12   \"+str(PMSSM_NN[7])+\"       # Ab \\n\")\n",
    "    f.write(\"  13   \"+str(PMSSM_NN[8])+\"       # Atau \\n\")\n",
    "    f.write(\"  23   \"+str(PMSSM_NN[5])+\"       # Mu \\n\")\n",
    "    f.write(\"  26   \"+str(PMSSM_NN[3])+\"       # Ma \\n\")\n",
    "    f.write(\"  31   \"+str(PMSSM_NN[15])+\"      # MeL \\n\")\n",
    "    f.write(\"  32   \"+str(PMSSM_NN[15])+\"      # MmuL \\n\")\n",
    "    f.write(\"  33   \"+str(PMSSM_NN[16])+\"      # MstauL \\n\")\n",
    "    f.write(\"  34   \"+str(PMSSM_NN[17])+\"      # MeR \\n\")\n",
    "    f.write(\"  35   \"+str(PMSSM_NN[17])+\"      # MmuR \\n\")\n",
    "    f.write(\"  36   \"+str(PMSSM_NN[18])+\"      # MstauR \\n\")\n",
    "    f.write(\"  41   \"+str(PMSSM_NN[9])+\"       # Mq1L \\n\")\n",
    "    f.write(\"  42   \"+str(PMSSM_NN[9])+\"       # Mq2L \\n\")\n",
    "    f.write(\"  43   \"+str(PMSSM_NN[10])+\"      # Mq3L \\n\")\n",
    "    f.write(\"  44   \"+str(PMSSM_NN[11])+\"      # MquR \\n\")\n",
    "    f.write(\"  45   \"+str(PMSSM_NN[11])+\"      # MqcR \\n\")\n",
    "    f.write(\"  46   \"+str(PMSSM_NN[13])+\"      # MqtR \\n\")\n",
    "    f.write(\"  47   \"+str(PMSSM_NN[12])+\"      # MqdR \\n\")\n",
    "    f.write(\"  48   \"+str(PMSSM_NN[12])+\"      # MqsR \\n\")\n",
    "    f.write(\"  49   \"+str(PMSSM_NN[14])+\"      # MqbR \\n\")\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46ebbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oracle(logits, PMSSM_range, ratio, succes_path):\n",
    "    \n",
    "    PMSSM_AL = np.add(np.multiply(logits,PMSSM_range[2]),PMSSM_range[0])\n",
    "    \n",
    "    write_slha(PMSSM_AL, \"Advanced NN/AL/pmssm_al.in\")    \n",
    "    \n",
    "    os.system('./SUSY/softsusy-4.1.9/softpoint.x leshouches < Advanced\\ NN/AL/pmssm_al.in > Advanced\\ NN/AL/pmssm_al.out')\n",
    "            \n",
    "          \n",
    "    if os.path.getsize('/home/mathis/Stage M1/Advanced NN/AL/pmssm_al.out') == 0 :\n",
    "        ratio[0]+=1\n",
    "        return 0\n",
    "        \n",
    "    with open('Advanced NN/AL/pmssm_al.out') as slha_out:\n",
    "        content = slha_out.read()\n",
    "        if 'SOFTSUSY problem' in content:\n",
    "            ratio[0]+=1\n",
    "            return 0\n",
    "        if 'LSP # Warning' in content:\n",
    "            ratio[0]+=1\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    ratio[1]+=1\n",
    "    commande = 'mv Advanced\\ NN/AL/pmssm_al.out '\n",
    "    nom_fichier = 'pmssm_al_succes_'+str(ratio[1])+'.out'\n",
    "    os.system(commande+succes_path+nom_fichier)\n",
    "    return 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6553a13",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be63031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 19)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               2000      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,201\n",
      "Trainable params: 12,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 19)                1919      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,219\n",
      "Trainable params: 32,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 11:00:04.269443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-14 11:00:04.269510: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-14 11:00:04.269545: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MPC): /proc/driver/nvidia/version does not exist\n",
      "2022-06-14 11:00:04.269891: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "path = 'Advanced NN/AL/AL_run/'\n",
    "folder_r = '2022-06-08-08:57:07_II'\n",
    "\n",
    "discriminator = tf.keras.models.load_model(path+folder_r+'/model_II.h5')\n",
    "discriminator.summary()\n",
    "\n",
    "\n",
    "fnc_activation = 'relu'\n",
    "fnc_activation_output = 'sigmoid'\n",
    "inputs = tf.keras.Input(shape=(100,), dtype='float32')\n",
    "x = keras.layers.Dense(100, trainable=True, activation=fnc_activation)(inputs)\n",
    "x = keras.layers.Dense(100, trainable=True, activation=fnc_activation)(x)\n",
    "x = keras.layers.Dense(100, trainable=True, activation=fnc_activation)(x)\n",
    "outputs = keras.layers.Dense(19, activation=fnc_activation_output)(x)\n",
    "generator = tf.keras.Model(inputs,outputs)\n",
    "generator.summary()\n",
    "\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction='sum_over_batch_size')\n",
    "#False pour une probabilité ie [0;1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df54be",
   "metadata": {},
   "source": [
    "# Initial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24227af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [00:06<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "a\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: (['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[-0.08175013,  0.08873218,  0.13267428, ...,  0.12386248,\n         0.02024953,  0.17318806],\n       [-0.00564371,  0.1717399 ,  0.15919787, ..., -0.16519573,\n        -0.03370659, -0.10048437],\n       [-0.08450729, -0.11112246,  0.16727582, ...,  0.15352985,\n         0.12015399, -0.0538227 ],\n       ...,\n       [ 0.0142646 , -0.17063317, -0.06185063, ...,  0.02419938,\n         0.01679154, -0.04848536],\n       [-0.04578717, -0.1140735 , -0.04452217, ..., -0.1375311 ,\n        -0.10571364, -0.16008948],\n       [-0.11205961,  0.02524815, -0.07972381, ..., -0.15729569,\n        -0.01647463,  0.16195562]], dtype=float32)>), (None, <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_1/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[ 0.04260685, -0.12584105, -0.06689312, ...,  0.08822665,\n        -0.03803831, -0.04221706],\n       [-0.10707572, -0.07081836, -0.02732874, ...,  0.09733754,\n        -0.10171729,  0.1527431 ],\n       [-0.16282645, -0.13662598,  0.11059296, ...,  0.06257693,\n        -0.12448281, -0.05071386],\n       ...,\n       [-0.16525458, -0.12122142,  0.07404418, ..., -0.01532811,\n        -0.1293478 ,  0.07689959],\n       [-0.07836512,  0.1602554 , -0.07543133, ..., -0.04199593,\n         0.02757466,  0.02562803],\n       [ 0.01403698, -0.13022839, -0.161852  , ...,  0.07640322,\n         0.04700392,  0.00997584]], dtype=float32)>), (None, <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_2/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[-0.12020965,  0.08975151,  0.12113625, ..., -0.03718825,\n        -0.05053291,  0.07734764],\n       [ 0.11818874, -0.00134647,  0.13282686, ...,  0.10857555,\n        -0.15152487, -0.04625764],\n       [-0.16601461,  0.00797826, -0.05697559, ...,  0.03591231,\n         0.12956014,  0.0632626 ],\n       ...,\n       [ 0.04706571,  0.02690183,  0.01064704, ..., -0.07919961,\n         0.09824133, -0.15054634],\n       [ 0.03993826,  0.11177078,  0.12826607, ..., -0.11187775,\n         0.11661676, -0.12379784],\n       [-0.05736224, -0.11594281,  0.1488542 , ..., -0.13565484,\n        -0.01315764, -0.02930406]], dtype=float32)>), (None, <tf.Variable 'dense_2/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_3/kernel:0' shape=(100, 19) dtype=float32, numpy=\narray([[-0.08289696, -0.10370987,  0.15135752, ..., -0.19547783,\n         0.03735025, -0.0771208 ],\n       [ 0.08228083, -0.18876612, -0.13976756, ...,  0.21864538,\n        -0.00734165,  0.15946458],\n       [ 0.22358914, -0.06264023,  0.10067917, ...,  0.12002172,\n         0.07525711, -0.19902012],\n       ...,\n       [ 0.20814605, -0.08403449,  0.00060083, ...,  0.056953  ,\n         0.07720764,  0.12715818],\n       [ 0.02188773,  0.18312214, -0.16591036, ...,  0.01131104,\n        -0.06952833, -0.13882211],\n       [ 0.15960692,  0.13025098, -0.1987512 , ...,  0.19864033,\n        -0.18422487,  0.172594  ]], dtype=float32)>), (None, <tf.Variable 'dense_3/bias:0' shape=(19,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>)).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         loss_evolution_g\u001b[38;5;241m.\u001b[39mappend(loss_g)\n\u001b[1;32m     50\u001b[0m         gradients_g \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss_g, generator\u001b[38;5;241m.\u001b[39mtrainable_weights)\n\u001b[0;32m---> 51\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients_g\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124méchecs:\u001b[39m\u001b[38;5;124m\"\u001b[39m,ratio_IT[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre de succes:\u001b[39m\u001b[38;5;124m\"\u001b[39m,ratio_IT[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:633\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[0;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m                     grads_and_vars,\n\u001b[1;32m    594\u001b[0m                     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    595\u001b[0m                     experimental_aggregate_gradients\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;124;03m\"\"\"Apply gradients to variables.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m  This is the second part of `minimize()`. It returns an `Operation` that\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;124;03m    RuntimeError: If called in a cross-replica context.\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m   var_list \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m (_, v) \u001b[38;5;129;01min\u001b[39;00m grads_and_vars]\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name):\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# Create iteration if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/optimizer_v2/utils.py:73\u001b[0m, in \u001b[0;36mfilter_empty_gradients\u001b[0;34m(grads_and_vars)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[1;32m     72\u001b[0m   variable \u001b[38;5;241m=\u001b[39m ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m _, v \u001b[38;5;129;01min\u001b[39;00m grads_and_vars],)\n\u001b[0;32m---> 73\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided `grads_and_vars` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrads_and_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vars_with_empty_grads:\n\u001b[1;32m     76\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m     77\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m when minimizing the loss. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using `model.compile()`, did you forget to provide a `loss`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument?\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     80\u001b[0m       ([v\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_with_empty_grads]))\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable: (['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'dense/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[-0.08175013,  0.08873218,  0.13267428, ...,  0.12386248,\n         0.02024953,  0.17318806],\n       [-0.00564371,  0.1717399 ,  0.15919787, ..., -0.16519573,\n        -0.03370659, -0.10048437],\n       [-0.08450729, -0.11112246,  0.16727582, ...,  0.15352985,\n         0.12015399, -0.0538227 ],\n       ...,\n       [ 0.0142646 , -0.17063317, -0.06185063, ...,  0.02419938,\n         0.01679154, -0.04848536],\n       [-0.04578717, -0.1140735 , -0.04452217, ..., -0.1375311 ,\n        -0.10571364, -0.16008948],\n       [-0.11205961,  0.02524815, -0.07972381, ..., -0.15729569,\n        -0.01647463,  0.16195562]], dtype=float32)>), (None, <tf.Variable 'dense/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_1/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[ 0.04260685, -0.12584105, -0.06689312, ...,  0.08822665,\n        -0.03803831, -0.04221706],\n       [-0.10707572, -0.07081836, -0.02732874, ...,  0.09733754,\n        -0.10171729,  0.1527431 ],\n       [-0.16282645, -0.13662598,  0.11059296, ...,  0.06257693,\n        -0.12448281, -0.05071386],\n       ...,\n       [-0.16525458, -0.12122142,  0.07404418, ..., -0.01532811,\n        -0.1293478 ,  0.07689959],\n       [-0.07836512,  0.1602554 , -0.07543133, ..., -0.04199593,\n         0.02757466,  0.02562803],\n       [ 0.01403698, -0.13022839, -0.161852  , ...,  0.07640322,\n         0.04700392,  0.00997584]], dtype=float32)>), (None, <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_2/kernel:0' shape=(100, 100) dtype=float32, numpy=\narray([[-0.12020965,  0.08975151,  0.12113625, ..., -0.03718825,\n        -0.05053291,  0.07734764],\n       [ 0.11818874, -0.00134647,  0.13282686, ...,  0.10857555,\n        -0.15152487, -0.04625764],\n       [-0.16601461,  0.00797826, -0.05697559, ...,  0.03591231,\n         0.12956014,  0.0632626 ],\n       ...,\n       [ 0.04706571,  0.02690183,  0.01064704, ..., -0.07919961,\n         0.09824133, -0.15054634],\n       [ 0.03993826,  0.11177078,  0.12826607, ..., -0.11187775,\n         0.11661676, -0.12379784],\n       [-0.05736224, -0.11594281,  0.1488542 , ..., -0.13565484,\n        -0.01315764, -0.02930406]], dtype=float32)>), (None, <tf.Variable 'dense_2/bias:0' shape=(100,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)>), (None, <tf.Variable 'dense_3/kernel:0' shape=(100, 19) dtype=float32, numpy=\narray([[-0.08289696, -0.10370987,  0.15135752, ..., -0.19547783,\n         0.03735025, -0.0771208 ],\n       [ 0.08228083, -0.18876612, -0.13976756, ...,  0.21864538,\n        -0.00734165,  0.15946458],\n       [ 0.22358914, -0.06264023,  0.10067917, ...,  0.12002172,\n         0.07525711, -0.19902012],\n       ...,\n       [ 0.20814605, -0.08403449,  0.00060083, ...,  0.056953  ,\n         0.07720764,  0.12715818],\n       [ 0.02188773,  0.18312214, -0.16591036, ...,  0.01131104,\n        -0.06952833, -0.13882211],\n       [ 0.15960692,  0.13025098, -0.1987512 , ...,  0.19864033,\n        -0.18422487,  0.172594  ]], dtype=float32)>), (None, <tf.Variable 'dense_3/bias:0' shape=(19,) dtype=float32, numpy=\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0.], dtype=float32)>))."
     ]
    }
   ],
   "source": [
    "os.system('rm -r Advanced\\ NN/AL/softsusy_succes_IT/')\n",
    "os.system('mkdir -p Advanced\\ NN/AL/softsusy_succes_IT')\n",
    "succes_path_IT = 'Advanced\\ NN/AL/softsusy_succes_IT/'\n",
    "\n",
    "ratio_IT = [0,0]\n",
    "\n",
    "\n",
    "K_size = 100\n",
    "train_step = 250\n",
    "\n",
    "loss_evolution_d = []\n",
    "loss_evolution_g = []\n",
    "\n",
    "for step , i in enumerate(range(train_step)):\n",
    "    print('Step {} / {}'.format(i+1,train_step))\n",
    "    \n",
    "    batch = [[rd.uniform(0,1) for j in range(100)] for k in range(K_size)]\n",
    "    batch = tf.convert_to_tensor(batch)\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        logits_g = generator(batch)\n",
    "    \n",
    "        pred_g = []\n",
    "        for j in tqdm(logits_g):\n",
    "            pred = Oracle(j, PMSSM_range, ratio_IT, succes_path_IT)\n",
    "            pred_g.append(pred)\n",
    "    \n",
    "\n",
    "        logits_d = discriminator(logits_g)\n",
    "        \n",
    "        \n",
    "        true = tf.convert_to_tensor([pred_g])\n",
    "        true = tf.transpose(true)      \n",
    "        true_prime = tf.cast(true, dtype=tf.float32)\n",
    "\n",
    "        loss_d = loss_fn(true_prime,logits_d)\n",
    "        loss_evolution_d.append(loss_d)\n",
    "        \n",
    "        gradients_d = tape.gradient(loss_d, discriminator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients_d, discriminator.trainable_weights))\n",
    "    \n",
    "        #loss_g = loss_d\n",
    "        loss_g = tf.math.abs(tf.math.subtract(1,loss_d))\n",
    "        #loss_g = tf.convert_to_tensor(1-ratio_IT[1]/(ratio_IT[1]+ratio_IT[0]))\n",
    "        print(loss_g)\n",
    "        print('a')\n",
    "        loss_evolution_g.append(loss_g)\n",
    "        \n",
    "        gradients_g = tape.gradient(loss_g, generator.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients_g, generator.trainable_weights))\n",
    "        \n",
    "    \n",
    "print(\"Nombre d'échecs:\",ratio_IT[0])\n",
    "print(\"Nombre de succes:\",ratio_IT[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(train_step)],loss_evolution_d,'ob')\n",
    "plt.plot([i for i in range(train_step)],loss_evolution_g,'or')\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935970d",
   "metadata": {},
   "source": [
    "# Récupération data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94816a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperation_slha(folder,filename,sus_mass,param,succes):\n",
    "    warning = 0\n",
    "    for j in range(succes):\n",
    "        file_name =folder+filename+str(j+1)+'.out' \n",
    "        \n",
    "        with open(file_name) as file:\n",
    "            for indice, ligne in enumerate(file):\n",
    "                if 'tanb' in ligne:\n",
    "                    indice_tanb = indice\n",
    "                \n",
    "                if 'M_1(MX)' in ligne:\n",
    "                    indices_M = [indice+i for i in range(3)]\n",
    "                if 'At(MX)' in ligne:\n",
    "                    indices_A = [indice+i for i in range(3)]\n",
    "                if 'mu(MX)' in ligne:\n",
    "                    indice_mu = indice\n",
    "                if 'mA(pole)' in ligne:\n",
    "                    indice_mA = indice\n",
    "                if 'meL(MX)' in ligne:\n",
    "                    indices_mlL = [indice+i for i in range(3)]\n",
    "                if 'meR(MX)' in ligne:\n",
    "                    indices_mlR = [indice+i for i in range(3)]\n",
    "                if 'mqL1(MX)' in ligne:\n",
    "                    indices_qL = [indice+i for i in range(3)]\n",
    "                if 'muR(MX)' in ligne:\n",
    "                    indices_q = [indice+i for i in range(6)]\n",
    "                    \n",
    "                if 'h0' in ligne:\n",
    "                    indices_higgs = [indice+i for i in range(2)]\n",
    "                if '~g' in ligne:\n",
    "                    indice_g = indice\n",
    "                if '~neutralino(1)' in ligne:\n",
    "                    indices_neutralino = [indice,indice+1,indice+3,indice+4]\n",
    "                if '~d_L' in ligne:\n",
    "                    indices_squarkL = [indice+i for i in range(6)]\n",
    "                if '~d_R' in ligne:\n",
    "                    indices_squarkR = [indice+i for i in range(6)]\n",
    "                if '~chargino(1)' in ligne:\n",
    "                    indices_chargino = [indice,indice+3]\n",
    "        \n",
    "        \n",
    "        file = open(file_name)\n",
    "        lignes = file.readlines()\n",
    "        \n",
    "        for step, i in enumerate(param[0]):\n",
    "            i.append(float(lignes[indices_M[step]].split()[1]))\n",
    "\n",
    "        for step, i in enumerate(param[1]):\n",
    "            i.append(float(lignes[indices_A[step]].split()[1]))  \n",
    "            \n",
    "        param[2].append(float(lignes[indice_mu].split()[1]))\n",
    "        \n",
    "        param[3].append(float(lignes[indice_mA].split()[1]))\n",
    "        \n",
    "        for step, i in enumerate(param[4]):\n",
    "            i.append(float(lignes[indices_mlL[step]].split()[1]))\n",
    "            \n",
    "        for step, i in enumerate(param[5]):\n",
    "            i.append(float(lignes[indices_mlR[step]].split()[1]))\n",
    "            \n",
    "        for step, i in enumerate(param[6]):\n",
    "            i.append(float(lignes[indices_qL[step]].split()[1]))\n",
    "        \n",
    "        for step, i in enumerate(param[7]):\n",
    "            i.append(float(lignes[indices_q[step]].split()[1]))\n",
    "            \n",
    "        param[8].append(float(lignes[indice_tanb].split()[1]))\n",
    "            \n",
    "        \n",
    "        sus_mass[0].append(float(lignes[indice_g].split()[1]))\n",
    "        \n",
    "        for step, i in enumerate(sus_mass[1]):\n",
    "            i.append(float(lignes[indices_neutralino[step]].split()[1]))\n",
    "            \n",
    "        for step, i in enumerate(sus_mass[2]):\n",
    "            i.append(float(lignes[indices_squarkL[step]].split()[1]))\n",
    "\n",
    "        for step, i in enumerate(sus_mass[3]):\n",
    "            i.append(float(lignes[indices_squarkR[step]].split()[1]))\n",
    "        \n",
    "        for step, i in enumerate(sus_mass[4]):\n",
    "            i.append(float(lignes[indices_higgs[step]].split()[1]))\n",
    "\n",
    "        for step, i in enumerate(sus_mass[5]):\n",
    "            i.append(float(lignes[indices_chargino[step]].split()[1]))        \n",
    "    \n",
    "    \n",
    "        with open(file_name) as file:\n",
    "            if 'LSP # Warning' in file.read():\n",
    "                warning+=1\n",
    "    return warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add38be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mass = []\n",
    "neutralino_mass = [[] for i in range(4)]   #[N1 , N2 , N3 , N4]\n",
    "squarkL_mass = [[] for i in range(6)]      #[d , u , s , c , b , t]\n",
    "squarkR_mass = [[] for i in range(6)]       \n",
    "higgs_mass = [[] for i in range(2)]        #[h0 , H0]\n",
    "chargino_mass = [[] for i in range(2)]     #[neutralino1 , neutralino2]\n",
    "\n",
    "M_param = [[] for i in range(3)]           #[M1 , M2 , M3]\n",
    "A_param = [[] for i in range(3)]           #[At , Ab , Atau]\n",
    "mu = []\n",
    "mA_param = []\n",
    "mlL_param = [[] for i in range(3)]         #[meL , mmuL , mtauL]\n",
    "mlR_param = [[] for i in range(3)]         #[meR , mmuR , mtauR]\n",
    "mqL_param = [[] for i in range(3)]         #[mqL1 , mqL2 , mqL3]\n",
    "quark_param = [[] for i in range(6)]       #[u , c , t , d , s , b]\n",
    "tanB = []\n",
    "\n",
    "sus_mass = [g_mass,neutralino_mass,squarkL_mass,squarkR_mass,higgs_mass,chargino_mass]\n",
    "param = [M_param,A_param,mu,mA_param,mlL_param,mlR_param,mqL_param,quark_param,tanB]\n",
    "\n",
    "folder = 'Advanced NN/AL/softsusy_succes_IT/'\n",
    "file_name = 'pmssm_al_succes_'\n",
    "warning = recuperation_slha(folder,file_name,sus_mass,param,ratio_IT[1])\n",
    "\n",
    "print('Nombre warning:',warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdd25eb",
   "metadata": {},
   "source": [
    "# Sauvegarde Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "\n",
    "folder_name = 'GAN_'+str(date.today())+'-'+str(datetime.now().time().strftime(\"%H:%M:%S\"))\n",
    "commande = 'mkdir -p Advanced\\ NN/AL/AL_run/'+folder_name\n",
    "os.system(commande)\n",
    "\n",
    "with open('Advanced NN/AL/AL_run/'+folder_name+'/config.txt', 'w') as f:\n",
    "    f.write('K: '+str(K_size)+'\\n')\n",
    "    f.write('train steps: '+str(train_step)+'\\n')\n",
    "    f.write(\"Nombre d'échecs: \"+str(ratio_IT[0])+'\\n')\n",
    "    f.write(\"Nombre de succes: \"+str(ratio_IT[1])+'\\n')\n",
    "    f.write(\"Nombre Warning LSP: \"+str(warning)+'\\n')\n",
    "    f.write('\\n')\n",
    "    f.write(\"Fonctions d'activations: \"+fnc_activation+'\\n')\n",
    "    f.write(\"Fonctions d'activations output: \"+fnc_activation_output+'\\n')\n",
    "    f.write('Learning rate:'+str(lr)+'\\n')\n",
    "    f.write('Optimizer: '+str(optimizer)+'\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('Generator:')\n",
    "    generator.summary(print_fn=lambda x: f.write(x + '\\n \\n'))\n",
    "    f.write('Discriminator:')\n",
    "    discriminator.summary(print_fn=lambda x: f.write(x + '\\n \\n'))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('Range:')\n",
    "    f.write('M1: '+str(PMSSM_range_big[0])+'\\n')\n",
    "    f.write('M2: '+str(PMSSM_range_big[1])+'\\n')\n",
    "    f.write('M3: '+str(PMSSM_range_big[2])+'\\n')\n",
    "    f.write('mA: '+str(PMSSM_range_big[3])+'\\n')\n",
    "    f.write('tanB: '+str(PMSSM_range_big[4])+'\\n')\n",
    "    f.write('mu: '+str(PMSSM_range_big[5])+'\\n')\n",
    "    f.write('At: '+str(PMSSM_range_big[6])+'\\n')\n",
    "    f.write('Ab: '+str(PMSSM_range_big[7])+'\\n')\n",
    "    f.write('Atau: '+str(PMSSM_range_big[8])+'\\n')\n",
    "    f.write('Mq1L: '+str(PMSSM_range_big[9])+'\\n')\n",
    "    f.write('Mq3L: '+str(PMSSM_range_big[10])+'\\n')\n",
    "    f.write('MuR: '+str(PMSSM_range_big[11])+'\\n')\n",
    "    f.write('dR: '+str(PMSSM_range_big[12])+'\\n')\n",
    "    f.write('MtR: '+str(PMSSM_range_big[13])+'\\n')\n",
    "    f.write('MbR: '+str(PMSSM_range_big[14])+'\\n')\n",
    "    f.write('MeL: '+str(PMSSM_range_big[15])+'\\n')\n",
    "    f.write('MtauL: '+str(PMSSM_range_big[16])+'\\n')\n",
    "    f.write('MeR: '+str(PMSSM_range_big[17])+'\\n')\n",
    "    f.write('MtauR: '+str(PMSSM_range_big[18])+'\\n')\n",
    "    \n",
    "\n",
    "generator.save('Advanced NN/AL/AL_run/'+folder_name+'/generator.h5')\n",
    "discriminator.save('Advanced NN/AL/AL_run/'+folder_name+'/discriminator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48281f",
   "metadata": {},
   "source": [
    "# Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f388374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histo(title, xlabel, ylabel, list_hist,bins, label, legend = False, histtype='bar', save=False, file_folder=''):\n",
    "    plt.figure()\n",
    "    for indice, hist in enumerate(list_hist):\n",
    "        plt.hist(hist,bins,label=label[indice],histtype=histtype)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if legend == True:\n",
    "        plt.legend()\n",
    "    if save == True:\n",
    "        plt.savefig(file_folder)\n",
    "    \n",
    "def subplot_histo(titre, xlabel, ylabel, x, y, share, subtitle, hist_list, label, legend = False, histtype='bar', save=False, file_folder=''):  \n",
    "    fig , ax = plt.subplots(x, y, sharex=share[0], sharey=share[1], constrained_layout = True)\n",
    "    plt.suptitle(titre)\n",
    "    fig.text(0.5, -0.05, xlabel, ha='center')\n",
    "    fig.text(-0.04, 0.5, ylabel, va='center', rotation='vertical')\n",
    "\n",
    "    for indice, hist in enumerate(hist_list): \n",
    "        compteur = 0\n",
    "        for i in range(x):\n",
    "            if y != 1:\n",
    "                for j in range(y):\n",
    "                    ax[i, j].hist(hist[compteur],100,histtype=histtype,label=label[indice])\n",
    "                    ax[i, j].set_title(subtitle[compteur])\n",
    "                    compteur+=1\n",
    "                    if legend == True:\n",
    "                        ax[i, j].legend()\n",
    "            else:\n",
    "                ax[i].hist(hist[compteur],100,histtype=histtype, label=label[indice])\n",
    "                ax[i].set_title(subtitle[compteur])\n",
    "                compteur+=1\n",
    "                if legend == True:\n",
    "                    ax[i].legend()\n",
    "    if save == True:\n",
    "        plt.savefig(file_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a141f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Advanced NN/AL/AL_run/'+folder_name\n",
    "\n",
    "file = '/histo_gluinos.svg'\n",
    "titre = r'Histogramme de la masse du gluino $\\widetilde{g}$'\n",
    "xlabel = 'Masse (GeV)'\n",
    "ylabel = 'N'\n",
    "plot_histo(titre,xlabel,ylabel,[g_mass],100,['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_neutralinos.svg'\n",
    "titre = 'Histogramme de la masse des neutralinos'\n",
    "subtitle = [r'$\\widetilde{\\chi_1^0}$',r'$\\widetilde{\\chi_2^0}$',r'$\\widetilde{\\chi_3^0}$',r'$\\widetilde{\\chi_4^0}$']\n",
    "share = [False,True]\n",
    "subplot_histo(titre,xlabel,ylabel,2,2,share,subtitle,[neutralino_mass],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_quarksL.svg'\n",
    "titre = 'Histogramme de la masse des quarks L'\n",
    "subtitle = [r'$\\widetilde{d}$',r'$\\widetilde{u}$',r'$\\widetilde{s}$',r'$\\widetilde{c}$',r'$\\widetilde{t}$',r'$\\widetilde{b}$']    \n",
    "subplot_histo(titre,xlabel,ylabel,2,3,share,subtitle,[squarkL_mass],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_quarksR.svg'\n",
    "titre = 'Histogramme de la masse des quarks R' \n",
    "subplot_histo(titre,xlabel,ylabel,2,3,share,subtitle,[squarkR_mass],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_higgs.svg'\n",
    "titre = 'Histogramme de la masse des bosons de Higgs'    \n",
    "subtitle = [r'$h_0$',r'$H_0$']\n",
    "subplot_histo(titre,xlabel,ylabel,2,1,share,subtitle,[higgs_mass],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_charginos.svg'\n",
    "titre = 'Histogramme de la masse des charginos'    \n",
    "subtitle = [r'$\\widetilde{\\chi_1^\\pm}$',r'$\\widetilde{\\chi_2^\\pm}$']\n",
    "subplot_histo(titre,xlabel,ylabel,2,1,share,subtitle,[chargino_mass],['NN'],save=True,file_folder=folder+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57f3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/histo_mu.svg'\n",
    "titre = r'Histogramme du paramètre $\\mu$'\n",
    "plot_histo(titre,xlabel,ylabel,[mu],100,['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_mquarks.svg'\n",
    "titre = 'Histogramme des paramètres de masse des quarks'\n",
    "subtitle = [r'$m_{\\widetilde{u_R}}$',r'$m_{\\widetilde{c_R}}$',r'$m_{\\widetilde{t_R}}$',r'$m_{\\widetilde{d_R}}$',r'$m_{\\widetilde{s_R}}$',r'$m_{\\widetilde{b_R}}$']\n",
    "subplot_histo(titre,xlabel,ylabel,2,3,share,subtitle,[quark_param],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_M.svg'\n",
    "titre = 'Histogramme des paramètres de masse'\n",
    "subtitle = [r'$M_1$',r'$M_2$',r'$M_3$']\n",
    "subplot_histo(titre,xlabel,ylabel,3,1,share,subtitle,[M_param],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_A.svg'\n",
    "titre = 'Histogramme des paramètres A'\n",
    "subtitle = [r'$A_t$',r'$A_b$',r'$A_\\tau$']\n",
    "subplot_histo(titre,xlabel,ylabel,3,1,share,subtitle,[A_param],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_mA.svg'\n",
    "titre = r'Histogramme du paramètre $M_A$'\n",
    "plot_histo(titre,xlabel,ylabel,[mA_param],100,['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_mlL.svg'\n",
    "titre = r'Histogramme des paramètres $m_{\\widetilde{f}}$'\n",
    "subtitle = [r'$m_{\\widetilde{e_L}}$',r'$m_{\\widetilde{\\mu_L}}$',r'$m_{\\widetilde{\\tau_L}}$',r'$m_{\\widetilde{e_R}}$',r'$m_{\\widetilde{\\mu_R}}$',r'$m_{\\widetilde{\\tau_R}}$']\n",
    "subplot_histo(titre,xlabel,ylabel,2,3,share,subtitle,[mlL_param+mlR_param],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_mqL.svg'\n",
    "titre = 'Histogramme des paramètres de masse'\n",
    "subtitle = [r'$m_{\\widetilde{q_{1L}}}$',r'$m_{\\widetilde{q_{2L}}}$',r'$m_{\\widetilde{q_{3L}}}$']\n",
    "subplot_histo(titre,xlabel,ylabel,3,1,share,subtitle,[mqL_param],['NN'],save=True,file_folder=folder+file)\n",
    "\n",
    "file = '/histo_tanb.svg'\n",
    "titre = r'Histogramme du paramètre $\\tan \\beta$'\n",
    "plot_histo(titre,xlabel,ylabel,[tanB],100,['NN'],save=True,file_folder=folder+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c92df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statisique du Neural Network')\n",
    "print(\"Nombre d'échecs:\",ratio_IT[0])\n",
    "print(\"Nombre de succes:\",ratio_IT[1])\n",
    "print('Nombre warning:', warning)\n",
    "print('Éfficacité du AL:',100*ratio_IT[1]/(ratio_IT[0]+ratio_IT[1]),'%')\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
