{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c73340",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60cca875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:02:29.652992: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-04 14:02:29.653019: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62889d1c",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "459c885e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 14:02:31.248852: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-04 14:02:31.248884: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-04 14:02:31.248904: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (MPC): /proc/driver/nvidia/version does not exist\n",
      "2022-05-04 14:02:31.249191: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[5,2],[1,3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ad75623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "707f554a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dtype:\",x.dtype)\n",
    "print(\"shape:\",x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80aedfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones(shape=(2,1)))\n",
    "print(tf.zeros(shape=(2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ea0563",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.46699458  0.4508374 ]\n",
      " [-0.8296199   0.928017  ]], shape=(2, 2), dtype=float32) \n",
      " tf.Tensor(\n",
      "[[ 6  1]\n",
      " [13  8]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(2,2), mean=0.0, stddev=1.0)\n",
    "y = tf.random.uniform(shape=(2,2), minval=0, maxval=20, dtype='int32')\n",
    "print(x,'\\n',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58766b4",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1802b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-0.22044593, -0.8775673 ],\n",
      "       [-1.3137823 , -1.0088197 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2,2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99ff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = tf.random.normal(shape=(2,2))\n",
    "a.assign(new_value)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i,j] == new_value[i,j] #assert verification egalit√©\n",
    "        \n",
    "added_value = tf.random.normal(shape=(2,2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i,j] == new_value[i,j] + added_value[i,j]\n",
    "\n",
    "subbed_value = tf.random.normal(shape=(2,2))\n",
    "a.assign_sub(subbed_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i,j] == new_value[i,j] + added_value[i,j] - subbed_value[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ab5e64",
   "metadata": {},
   "source": [
    "# Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1330cf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.01602904  1.2334924 ]\n",
      " [-1.1652095  -0.8601498 ]], shape=(2, 2), dtype=float32) \n",
      " tf.Tensor(\n",
      "[[ 1.0650553  -1.3814703 ]\n",
      " [-0.5567174   0.61867106]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[6.0650554 0.6185297]\n",
      " [0.4432826 3.618671 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2,2))\n",
    "b = tf.random.normal(shape=(2,2))\n",
    "print(a,'\\n',b)\n",
    "\n",
    "c = a+b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)\n",
    "\n",
    "aprime = tf.constant([[5,2],[1,3]], dtype='float32')\n",
    "aprime = aprime+b\n",
    "print(aprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9f785",
   "metadata": {},
   "source": [
    "# Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afeaf145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.9277204   0.30263886]\n",
      " [-0.89379     0.3658173 ]], shape=(2, 2), dtype=float32) \n",
      " tf.Tensor(\n",
      "[[-0.8010214  0.6442072]\n",
      " [-1.212875  -0.9206741]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.75690037  0.4252017 ]\n",
      " [-0.5932389   0.36925572]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.75690037  0.42520174]\n",
      " [-0.5932389   0.36925572]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2,2))\n",
    "b = tf.random.normal(shape=(2,2))\n",
    "print(a,'\\n',b)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)\n",
    "    c = tf.sqrt(tf.square(a)+tf.square(b))\n",
    "    dc_da = tape.gradient(c,a)\n",
    "    print(dc_da)\n",
    "    \n",
    "print(a*tf.pow((tf.square(a)+tf.square(b)),-0.5)) #pow(x,y)=x^y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f598f67",
   "metadata": {},
   "source": [
    "Variable sont watch automatiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b35a044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.75690037  0.4252017 ]\n",
      " [-0.5932389   0.36925572]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt((tf.square(a)+tf.square(b)))\n",
    "    dc_da = tape.gradient(c,a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e3956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.34846005 1.150965  ]\n",
      " [0.43014455 0.8717679 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.34846008 1.150965  ]\n",
      " [0.43014458 0.8717679 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a)+tf.square(b))\n",
    "        dc_da = tape.gradient(c,a)\n",
    "    d2c_da2= outer_tape.gradient(dc_da,a)\n",
    "    print(d2c_da2)\n",
    "    \n",
    "print(tf.square(b)/pow(tf.square(a)+tf.square(b),3/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0a0c5",
   "metadata": {},
   "source": [
    "# Keras layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb201b",
   "metadata": {},
   "source": [
    "Classe self/super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e2a7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed: 10\n",
      "is a mammal: True\n",
      "feed the cat?: True\n"
     ]
    }
   ],
   "source": [
    "class Animal(object):\n",
    "     def __init__(self, speed, is_mammal):\n",
    "          self.speed = speed\n",
    "          self.is_mammal = is_mammal\n",
    "\n",
    "class Cat(Animal):\n",
    "     def __init__(self, is_hungry):\n",
    "          super().__init__(10, True)\n",
    "          self.is_hungry = is_hungry\n",
    "\n",
    "barry = Cat(True)\n",
    "print(f\"speed: {barry.speed}\")\n",
    "print(f\"is a mammal: {barry.is_mammal}\")\n",
    "print(f\"feed the cat?: {barry.is_hungry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da854f",
   "metadata": {},
   "source": [
    "Simple Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d31214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x +b\"\"\"\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim,units),dtype='float32'), trainable=True,)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),dtype='float32'),trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3c976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.01935388, -0.0521484 ,  0.03543316,  0.00357566],\n",
      "       [-0.06403334, -0.03481334,  0.05175444, -0.12122536]],\n",
      "      dtype=float32)>\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.04467946 -0.08696175  0.0871876  -0.1176497 ]\n",
      " [-0.04467946 -0.08696175  0.0871876  -0.1176497 ]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "units=4\n",
    "input_dim=2\n",
    "linear_layer = Linear(units, input_dim)\n",
    "print(linear_layer.w)\n",
    "\n",
    "one=tf.ones((2,input_dim))\n",
    "print(one)\n",
    "\n",
    "y = linear_layer(one)\n",
    "assert y.shape == (2,units)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "619b4fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.01935388, -0.0521484 ,  0.03543316,  0.00357566],\n",
      "       [-0.06403334, -0.03481334,  0.05175444, -0.12122536]],\n",
      "      dtype=float32)>, <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]\n",
    "print(linear_layer.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea49bd",
   "metadata": {},
   "source": [
    "# Layer wieght creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2133e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03073402  0.15474497  0.04661263 -0.05894125]\n",
      " [-0.03073402  0.15474497  0.04661263 -0.05894125]], shape=(2, 4), dtype=float32)\n",
      "\n",
      "\n",
      "[<tf.Variable 'linear_v2/Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.00136241,  0.06968736,  0.04097953, -0.04252573],\n",
      "       [-0.02175428,  0.06245536,  0.02777581,  0.01807622]],\n",
      "      dtype=float32)>, <tf.Variable 'linear_v2/Variable:0' shape=(4,) dtype=float32, numpy=array([-0.01034215,  0.02260225, -0.02214271, -0.03449174], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class LinearV2(keras.layers.Layer):\n",
    "    \"\"\"y = w.x +b\"\"\"\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(LinearV2, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer='random_normal',trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer='random_normal', trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)+self.b\n",
    "    \n",
    "linear_layerV2 = LinearV2(4)\n",
    "\n",
    "\n",
    "y = linear_layerV2(tf.ones((2,2)))\n",
    "print(y)\n",
    "print('\\n')\n",
    "print(linear_layerV2.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7ac65",
   "metadata": {},
   "source": [
    "# Layer Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d1ead4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.3959009647369385\n",
      "Step: 100 Loss: 2.2080469131469727\n",
      "Step: 200 Loss: 2.2266125679016113\n",
      "Step: 300 Loss: 2.004094123840332\n",
      "Step: 400 Loss: 1.976342797279358\n",
      "Step: 500 Loss: 1.9257280826568604\n",
      "Step: 600 Loss: 1.8282246589660645\n",
      "Step: 700 Loss: 1.7980353832244873\n",
      "Step: 800 Loss: 1.6211016178131104\n",
      "Step: 900 Loss: 1.6001383066177368\n"
     ]
    }
   ],
   "source": [
    "#dataset\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000,784).astype('float32')/255,y_train))\n",
    "dataset=dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "\n",
    "linear_layer = LinearV2(10)\n",
    "\n",
    "#loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = linear_layer(x) #exp value\n",
    "        loss = loss_fn(y,logits) #loss value (th,exp)\n",
    "\n",
    "    gradients = tape.gradient(loss,linear_layer.trainable_weights) #gradient du loss\n",
    "    #update des wieghts de notre layer\n",
    "    optimizer.apply_gradients(zip(gradients,linear_layer.trainable_weights)) \n",
    "    \n",
    "    if step % 100 ==0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f4f94",
   "metadata": {},
   "source": [
    "# Multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ca7a167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = LinearV2(32)\n",
    "        self.linear_2 = LinearV2(32)\n",
    "        self.linear_3 = LinearV2(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = MLP()\n",
    "\n",
    "y = mlp(tf.ones(shape=(3,64)))\n",
    "\n",
    "assert len(mlp.weights) == 6\n",
    "\n",
    "#print(mlp.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61d72b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = keras.Sequential([keras.layers.Dense(32, activation=tf.nn.relu),keras.layers.Dense(32, activation=tf.nn.relu),keras.layers.Dense(10),])\n",
    "\n",
    "y = mlp(tf.ones(shape=(3,64)))\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdea9cd",
   "metadata": {},
   "source": [
    "# Tracking losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b5516b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss\"\"\"\n",
    "    \n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate*tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8ff27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.20319775>]\n"
     ]
    }
   ],
   "source": [
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = LinearV2(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = LinearV2(10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10,10)))\n",
    "\n",
    "print(mlp.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c403b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 5.765660285949707 Loss_nc 2.3174777030944824\n",
      "Step: 100 Loss: 2.5629031658172607 Loss_nc 2.300293445587158\n",
      "Step: 200 Loss: 2.4214224815368652 Loss_nc 2.3162569999694824\n",
      "Step: 300 Loss: 2.3500871658325195 Loss_nc 2.301677703857422\n",
      "Step: 400 Loss: 2.3552515506744385 Loss_nc 2.310880661010742\n",
      "Step: 500 Loss: 2.3519856929779053 Loss_nc 2.297109603881836\n",
      "Step: 600 Loss: 2.3236260414123535 Loss_nc 2.3006980419158936\n",
      "Step: 700 Loss: 2.315823793411255 Loss_nc 2.3007073402404785\n",
      "Step: 800 Loss: 2.3179702758789062 Loss_nc 2.3000335693359375\n",
      "Step: 900 Loss: 2.314863681793213 Loss_nc 2.3001811504364014\n"
     ]
    }
   ],
   "source": [
    "#losses correspond au dernier appel\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10,10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10,10)))\n",
    "assert len(mlp.losses) == 1 #pas d'accumulation\n",
    "\n",
    "\n",
    "#dataset\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000,784).astype('float32')/255,y_train))\n",
    "dataset=dataset.shuffle(buffer_size=1024).batch(64)\n",
    "print(dataset)\n",
    "\n",
    "mlp = SparseMLP()\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mlp(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "        loss += sum(mlp.losses)\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_nc = mlp(x)\n",
    "        loss_nc = loss_fn(y, logits_nc)\n",
    "        gradients_nc = tape.gradient(loss_nc, mlp.trainable_weights)\n",
    "        \n",
    "    optimizer.apply_gradients(zip(gradients_nc, mlp.trainable_weights))\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss), \"Loss_nc\", float(loss_nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fc57ea",
   "metadata": {},
   "source": [
    "# Training metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9437afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0\n",
      "Loss: 2.4136898517608643 Total running accuracy so far: 0.078\n",
      "Epoch: 0 Step: 200\n",
      "Loss: 0.36844196915626526 Total running accuracy so far: 0.738\n",
      "Epoch: 0 Step: 400\n",
      "Loss: 0.2631455659866333 Total running accuracy so far: 0.820\n",
      "Epoch: 0 Step: 600\n",
      "Loss: 0.30521291494369507 Total running accuracy so far: 0.853\n",
      "Epoch: 0 Step: 800\n",
      "Loss: 0.4227861762046814 Total running accuracy so far: 0.871\n",
      "Epoch: 1 Step: 0\n",
      "Loss: 0.3909187912940979 Total running accuracy so far: 0.881\n",
      "Epoch: 1 Step: 200\n",
      "Loss: 0.34021520614624023 Total running accuracy so far: 0.892\n",
      "Epoch: 1 Step: 400\n",
      "Loss: 0.2911882996559143 Total running accuracy so far: 0.899\n",
      "Epoch: 1 Step: 600\n",
      "Loss: 0.129229336977005 Total running accuracy so far: 0.905\n",
      "Epoch: 1 Step: 800\n",
      "Loss: 0.2980092763900757 Total running accuracy so far: 0.910\n"
     ]
    }
   ],
   "source": [
    "#initialisation d'un objet metrique\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(32,activation='relu'),keras.layers.Dense(32,activation='relu'),keras.layers.Dense(10),])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    for step,(x,y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss_value = loss_fn(y,logits)\n",
    "            \n",
    "        accuracy.update_state(y,logits)\n",
    "        \n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\",epoch,\"Step:\",step)\n",
    "            print(\"Loss:\",float(loss_value),\"Total running accuracy so far: %.3f\" % accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce0ca7d",
   "metadata": {},
   "source": [
    "# Compiled functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "979c6f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.351717948913574 Running accuracy: 0.047\n",
      "Step: 100 Loss: 0.6290731430053711 Running accuracy: 0.639\n",
      "Step: 200 Loss: 0.4065704941749573 Running accuracy: 0.752\n",
      "Step: 300 Loss: 0.3158191442489624 Running accuracy: 0.796\n",
      "Step: 400 Loss: 0.2985181510448456 Running accuracy: 0.826\n",
      "Step: 500 Loss: 0.1950078010559082 Running accuracy: 0.844\n",
      "Step: 600 Loss: 0.3343420624732971 Running accuracy: 0.857\n",
      "Step: 700 Loss: 0.16793304681777954 Running accuracy: 0.867\n",
      "Step: 800 Loss: 0.21912036836147308 Running accuracy: 0.874\n",
      "Step: 900 Loss: 0.08259598165750504 Running accuracy: 0.882\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(32,activation='relu'),keras.layers.Dense(32,activation='relu'),keras.layers.Dense(10),])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "#initialisation d'un objet metrique\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function\n",
    "def train_on_batch(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y,logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    accuracy.update_state(y,logits)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    return loss\n",
    "\n",
    "\n",
    "#dataset\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train.reshape(60000,784).astype('float32')/255,y_train))\n",
    "dataset=dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "\n",
    "for step, (x,y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x,y)\n",
    "    if step % 100 ==0:\n",
    "        print('Step:',step,'Loss:',float(loss),\"Running accuracy: %.3f\" % accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df86e6b",
   "metadata": {},
   "source": [
    "# Interference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f5ddfa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02598498 -0.06834267  0.02753705  0.02624538 -0.05068519 -0.14904755\n",
      "   0.11757778  0.00647833  0.03034545  0.0530154 ]\n",
      " [ 0.03507911 -0.09084155  0.00749075  0.00236058 -0.03793593 -0.1258577\n",
      "   0.16628413 -0.02122709  0.05818252  0.07996084]], shape=(2, 10), dtype=float32) \n",
      " tf.Tensor(\n",
      "[[ 0.04354772 -0.06988318  0.03963013  0.043432   -0.03720988 -0.09483258\n",
      "   0.12644911 -0.02061515  0.0645809   0.05974258]\n",
      " [ 0.04354772 -0.06988318  0.03963013  0.043432   -0.03720988 -0.09483258\n",
      "   0.12644911 -0.02061515  0.0645809   0.05974258]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self,rate):\n",
    "        super(Dropout,self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self,inputs,training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs,rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "class MLPWithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout,self).__init__()\n",
    "        self.linear_1 = LinearV2(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = LinearV2(10)\n",
    "        \n",
    "    def call(self,inputs,training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x,training=training)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "\n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2,2)),training=True)\n",
    "y_test = mlp(tf.ones((2,2)),training=False)\n",
    "\n",
    "print(y_train,'\\n',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14434755",
   "metadata": {},
   "source": [
    "# Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23a18ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(16,),dtype='float32')\n",
    "\n",
    "x = LinearV2(32)(inputs)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = LinearV2(10)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,outputs)\n",
    "\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "y = model(tf.ones((2,16)))\n",
    "assert y.shape == (2,10)\n",
    "\n",
    "y = model(tf.ones((2,16)), training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "150d5df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.4062 - sparse_categorical_accuracy: 0.8849\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1830 - sparse_categorical_accuracy: 0.9465\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1527 - sparse_categorical_accuracy: 0.9549\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,506\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(784,), dtype=\"float32\")\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(10)(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),optimizer=keras.optimizers.Adam(learning_rate=1e-3),metrics=[keras.metrics.SparseCategoricalAccuracy()],)\n",
    "\n",
    "model.fit(dataset, epochs=2)\n",
    "model.predict(dataset)\n",
    "model.evaluate(dataset)\n",
    "\n",
    "model.summary() #etat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb5c14ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.3929 - accuracy: 0.8057\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 0.1982 - accuracy: 0.9376\n",
      "Model: \"custom_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32)                25120     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,510\n",
      "Trainable params: 26,506\n",
      "Non-trainable params: 4\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
    "        self.accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        x,y=data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training = True)\n",
    "            loss = self.loss_fn(y,y_pred)\n",
    "        gradients = tape.gradient(loss,self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients,self.trainable_weights))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.accuracy.update_state(y,y_pred)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.accuracy.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.accuracy]\n",
    "    \n",
    "\n",
    "inputs = tf.keras.Input(shape=(784,), dtype=\"float32\")\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(inputs)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "outputs = keras.layers.Dense(10)(x)\n",
    "\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "model.compile()\n",
    "model.fit(dataset, epochs=2)\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
